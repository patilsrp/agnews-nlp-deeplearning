# AG-News Topic Classifier using BiLSTM and DistilBERT
## Mini-Project Report

---

## ğŸ“Œ **Title**
**AG-News Topic Classifier using BiLSTM and DistilBERT: A Comparative Study of Deep Learning Approaches for Text Classification**

---

## ğŸ¯ **Scope**

### **Input and Output Interfaces**

**Input Interfaces:**
- **Single Text Input**: News headlines or short paragraphs entered via web interface
- **Batch Processing**: CSV file upload containing multiple text samples in 'text' column
- **Interactive Notebook**: Direct text input in Jupyter notebook cells
- **API Endpoints**: RESTful API accepting JSON payloads with text data

**Output Interfaces:**
- **Predicted Category**: One of four classes (World, Sports, Business, Science/Technology)
- **Confidence Scores**: Softmax probability distribution across all classes
- **Model Comparison**: Side-by-side predictions from both DistilBERT and BiLSTM models
- **Attention Visualization**: Word importance highlighting for BiLSTM model
- **Performance Metrics**: Accuracy, precision, recall, F1-score, and confusion matrices
- **Interactive Web Dashboard**: Real-time classification with confidence indicators

### **Source of Text Corpus and Size**

**Dataset**: AG News Classification Dataset
- **Source**: [Kaggle - AG News Classification Dataset](https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset)
- **Original Source**: AG's news topic classification dataset from academic research
- **Training Samples**: 120,000 news articles
- **Test Samples**: 7,600 news articles
- **Total Dataset Size**: 127,600 labeled samples
- **Text Characteristics**:
  - Average text length: 227 characters
  - Maximum text length: 951 characters
  - Language: English
  - Format: Title + Description combined
- **Class Distribution**: Balanced dataset with 25% samples per class
- **Classes**:
  1. **World** (Politics, International affairs)
  2. **Sports** (Athletics, competitions) 
  3. **Business** (Economy, finance, corporate news)
  4. **Science/Technology** (Scientific discoveries, tech innovations)

### **Major Modules/Functionalities**

| Module | Functionality | Implementation |
|--------|---------------|----------------|
| **Data Loader** | Load, split, and prepare AG News dataset | `src/data/data_loader.py` |
| **Text Preprocessing** | Clean, tokenize, and encode text inputs | Custom preprocessing pipelines |
| **BiLSTM Model** | Bidirectional LSTM with attention mechanism | `src/models/bilstm_attention.py` |
| **DistilBERT Model** | Transformer-based fine-tuning | `src/models/distilbert_classifier.py` |
| **Training Engine** | Model training with metrics tracking | `src/training/trainer.py` |
| **Evaluation System** | Performance analysis and comparison | Built-in evaluation functions |
| **Visualization Tools** | Training curves, confusion matrices, attention maps | Matplotlib/Seaborn integration |
| **Web API** | FastAPI-based REST interface | `src/api/app.py` |
| **Interactive Notebook** | Complete pipeline in Jupyter format | `AG_News_Classification.ipynb` |
| **Model Explainability** | LIME/SHAP integration for interpretability | `src/utils/explainer.py` |

### **Text Pre-processing Steps Applied**

| Step | Description | Implementation | Purpose |
|------|-------------|----------------|---------|
| **Text Combination** | Merge title and description fields | `text = title + ' ' + description` | Create unified input |
| **Lowercasing** | Convert to lowercase (BiLSTM only) | `text.lower()` | Normalize case variations |
| **URL Removal** | Remove web URLs and links | `re.sub(r'http\S+\|www\S+\|https\S+', '', text)` | Clean noise |
| **Punctuation Cleaning** | Remove special characters | `re.sub(r'[^\w\s]', '', text)` | Focus on words |
| **Whitespace Normalization** | Standardize spacing | `re.sub(r'\s+', ' ', text).strip()` | Consistent formatting |
| **Tokenization** | Convert text to tokens | DistilBERT: HuggingFace tokenizer<br>BiLSTM: Word-level splitting | Prepare for model input |
| **Sequence Padding/Truncation** | Fix sequence length to 128 | Padding with zeros, truncation at max_length | Uniform tensor sizes |
| **Label Encoding** | Map class indices (1-4) to (0-3) | `{1:0, 2:1, 3:2, 4:3}` | Zero-indexed classification |
| **Vocabulary Building** | Create word-to-index mapping (BiLSTM) | Top 10,000 frequent words | Efficient encoding |
| **Attention Masks** | Binary masks for valid tokens (DistilBERT) | Generated by tokenizer | Ignore padding tokens |

### **Deep Learning Techniques Implemented**

#### **1. BiLSTM with Attention Mechanism**
- **Architecture**: Bidirectional Long Short-Term Memory network
- **Embedding Layer**: 100-dimensional word embeddings
- **LSTM Configuration**: 
  - Hidden dimension: 128
  - Number of layers: 2
  - Bidirectional: True
  - Dropout: 0.3
- **Attention Mechanism**: Linear attention for word importance weighting
- **Classification Head**: Dense layers with ReLU activation and dropout
- **Vocabulary Size**: 10,000 most frequent words
- **Parameters**: 1,664,453 trainable parameters
- **Optimizer**: Adam with learning rate 1e-3

#### **2. DistilBERT Fine-tuning**
- **Base Model**: `distilbert-base-uncased` from Hugging Face
- **Architecture**: 6-layer transformer encoder (distilled from BERT)
- **Fine-tuning Strategy**: Full model fine-tuning with linear classification head
- **Configuration**:
  - Hidden size: 768
  - Attention heads: 12
  - Sequence length: 128 tokens
  - Parameters: ~66 million
- **Optimizer**: AdamW with learning rate 2e-5
- **Loss Function**: Cross-entropy loss
- **Regularization**: Weight decay and dropout

---

## ğŸ—ï¸ **Working of Project - Architecture**

### **System Architecture Diagram**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        AG News Dataset                          â”‚
â”‚                   (120K train + 7.6K test)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Text Preprocessing                            â”‚
â”‚  â€¢ Cleaning â€¢ Tokenization â€¢ Encoding â€¢ Padding               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚                             â”‚
               â–¼                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      BiLSTM Model       â”‚        â”‚    DistilBERT Model     â”‚
â”‚                         â”‚        â”‚                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   Embedding     â”‚   â”‚        â”‚  â”‚  Transformer    â”‚   â”‚
â”‚  â”‚    Layer        â”‚   â”‚        â”‚  â”‚    Encoder      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚           â”‚             â”‚        â”‚           â”‚             â”‚
â”‚           â–¼             â”‚        â”‚           â–¼             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ BiLSTM Layers   â”‚   â”‚        â”‚  â”‚ Classification  â”‚   â”‚
â”‚  â”‚   (Hidden:128)  â”‚   â”‚        â”‚  â”‚     Head        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚           â”‚             â”‚        â”‚           â”‚             â”‚
â”‚           â–¼             â”‚        â”‚           â–¼             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   Attention     â”‚   â”‚        â”‚  â”‚    Softmax      â”‚   â”‚
â”‚  â”‚   Mechanism     â”‚   â”‚        â”‚  â”‚   Probabilities â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚           â”‚             â”‚        â”‚           â”‚             â”‚
â”‚           â–¼             â”‚        â”‚           â–¼             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚        â”‚           â”‚             â”‚
â”‚  â”‚ Classification  â”‚   â”‚        â”‚           â”‚             â”‚
â”‚  â”‚     Head        â”‚   â”‚        â”‚           â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚        â”‚           â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                                â”‚
              â–¼                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Model Evaluation                             â”‚
â”‚            â€¢ Accuracy â€¢ Precision â€¢ Recall â€¢ F1-Score          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Model Comparison                              â”‚
â”‚        â€¢ Performance Analysis â€¢ Visualization â€¢ Insights       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Deployment Interface                           â”‚
â”‚     â€¢ Web API â€¢ Interactive UI â€¢ Batch Processing             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Workflow Explanation**

#### **Phase 1: Data Preparation**
1. **Dataset Loading**: Load AG News CSV files containing class labels, titles, and descriptions
2. **Text Combination**: Merge title and description fields to create comprehensive input text
3. **Preprocessing Pipeline**: Apply cleaning, normalization, and tokenization
4. **Data Splitting**: Create train (90%), validation (10%), and test sets with stratification
5. **Encoding**: Convert text to numerical representations suitable for each model

#### **Phase 2: Model Development**

**BiLSTM Pipeline:**
1. **Vocabulary Building**: Create word-to-index mapping from training corpus
2. **Embedding**: Map words to dense 100-dimensional vectors
3. **Bidirectional Processing**: Process sequences forward and backward through LSTM layers
4. **Attention Application**: Compute attention weights to focus on important words
5. **Context Aggregation**: Weighted sum of LSTM outputs using attention scores
6. **Classification**: Feed context vector through dense layers to predict class

**DistilBERT Pipeline:**
1. **Tokenization**: Use DistilBERT tokenizer for subword tokenization
2. **Encoding**: Convert tokens to input IDs and attention masks
3. **Transformer Processing**: Pass through 6 transformer layers with self-attention
4. **Pooling**: Extract [CLS] token representation as sentence embedding
5. **Classification**: Linear layer maps pooled output to class probabilities

#### **Phase 3: Training & Optimization**
1. **Loss Computation**: Cross-entropy loss for both models
2. **Backpropagation**: Gradient computation and parameter updates
3. **Validation Monitoring**: Track accuracy on validation set each epoch
4. **Early Stopping**: Prevent overfitting using validation performance
5. **Model Checkpointing**: Save best performing model states

#### **Phase 4: Evaluation & Analysis**
1. **Test Set Evaluation**: Compute final performance metrics
2. **Confusion Matrix Generation**: Analyze per-class performance
3. **Statistical Analysis**: Compare models using various metrics
4. **Visualization**: Generate training curves and performance charts
5. **Error Analysis**: Identify challenging cases and model limitations

#### **Phase 5: Deployment**
1. **Model Serialization**: Save trained models for inference
2. **API Development**: Create REST endpoints for text classification
3. **Web Interface**: Build interactive dashboard for user testing
4. **Batch Processing**: Enable CSV file upload and processing
5. **Documentation**: Generate comprehensive usage guides

---

## ğŸ”— **GitHub Repository Link**
**Repository**: [https://github.com/your-username/agnews-nlp-deeplearning](https://github.com/your-username/agnews-nlp-deeplearning)

**Repository Structure:**
- Complete source code with modular architecture
- Trained model checkpoints (distilbert_model.pth, bilstm_model.pth)
- Comprehensive Jupyter notebook (AG_News_Classification.ipynb)
- Training results and analysis (print.pdf)
- Environment configuration (environment.yml)
- Detailed documentation (README.md)
- Web API implementation (run_api.py)

---

## ğŸ“š **List of References**

### **Academic Papers & Research**
1. Sanh, V., Debut, L., Chaumond, J., & Wolf, T. (2019). *DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter*. arXiv preprint arXiv:1910.01108.
2. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). *Attention is all you need*. Advances in neural information processing systems, 30.
3. Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). *BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding*. arXiv preprint arXiv:1810.04805.
4. Hochreiter, S., & Schmidhuber, J. (1997). *Long short-term memory*. Neural computation, 9(8), 1735-1780.

### **Documentation & Tutorials**
5. **PyTorch Documentation**: [https://pytorch.org/docs/stable/index.html](https://pytorch.org/docs/stable/index.html)
6. **Hugging Face Transformers Documentation**: [https://huggingface.co/docs/transformers/index](https://huggingface.co/docs/transformers/index)
7. **Scikit-learn User Guide**: [https://scikit-learn.org/stable/user_guide.html](https://scikit-learn.org/stable/user_guide.html)
8. **FastAPI Documentation**: [https://fastapi.tiangolo.com/](https://fastapi.tiangolo.com/)

### **Dataset & Resources**
9. **AG News Dataset**: [https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset](https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset)
10. **Google Colab**: [https://colab.research.google.com/](https://colab.research.google.com/)
11. **Conda Environment Management**: [https://docs.conda.io/en/latest/](https://docs.conda.io/en/latest/)

### **Video Tutorials & Courses**
12. **Stanford CS224N: Natural Language Processing with Deep Learning**: [http://web.stanford.edu/class/cs224n/](http://web.stanford.edu/class/cs224n/)
13. **PyTorch Tutorials - Text Classification**: [https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html](https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html)
14. **Hugging Face Course**: [https://huggingface.co/course](https://huggingface.co/course)

### **Books**
15. Jurafsky, D., & Martin, J. H. (2023). *Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition* (3rd edition).
16. Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep Learning*. MIT Press.
17. Chollet, F. (2021). *Deep Learning with Python* (2nd edition). Manning Publications.

---

## ğŸ¤– **List of AI Tools/Applications Used**

### **1. Google Colab**
**Description**: Cloud-based Jupyter notebook environment with free GPU access
**Tasks Performed**:
- Model training with Tesla T4 GPU acceleration
- Interactive development and experimentation
- Collaborative notebook sharing and execution
- Automatic dependency management
- Resource monitoring and usage optimization

### **2. Hugging Face Transformers**
**Description**: State-of-the-art pre-trained transformer models library
**Tasks Performed**:
- Loading pre-trained DistilBERT model (`distilbert-base-uncased`)
- Tokenization using DistilBertTokenizer
- Model fine-tuning for sequence classification
- Automatic model optimization and deployment
- Integration with PyTorch training pipelines

### **3. PyTorch Framework**
**Description**: Deep learning framework for building and training neural networks
**Tasks Performed**:
- BiLSTM model architecture implementation
- Custom dataset and dataloader creation
- Training loop development with automatic differentiation
- Model serialization and checkpoint management
- GPU acceleration and memory optimization

### **4. Weights & Biases (Optional)**
**Description**: MLOps platform for experiment tracking and model management
**Tasks Performed**:
- Training metrics logging and visualization
- Hyperparameter sweep and optimization
- Model versioning and artifact storage
- Collaborative experiment sharing
- Performance comparison across runs

### **5. LIME (Local Interpretable Model-agnostic Explanations)**
**Description**: Model explainability tool for understanding predictions
**Tasks Performed**:
- Individual prediction explanation
- Feature importance visualization
- Text highlighting for influential words
- Model behavior analysis
- Trust and transparency enhancement

### **6. Claude Code (AI Assistant)**
**Description**: AI-powered coding assistant for development support
**Tasks Performed**:
- Code structure planning and organization
- Implementation guidance and best practices
- Debugging and error resolution
- Documentation generation and formatting
- Architecture design and optimization suggestions

### **7. GitHub Copilot (Optional)**
**Description**: AI pair programmer for code completion and generation
**Tasks Performed**:
- Boilerplate code generation
- Function implementation suggestions
- Unit test creation
- Code refactoring assistance
- Documentation string generation

### **8. TensorBoard/Matplotlib**
**Description**: Visualization tools for training monitoring and results analysis
**Tasks Performed**:
- Training loss and accuracy curve plotting
- Confusion matrix heatmap generation
- Model architecture visualization
- Performance comparison charts
- Statistical analysis and reporting

### **9. FastAPI Framework**
**Description**: Modern web framework for building APIs with automatic documentation
**Tasks Performed**:
- REST API endpoint development
- Automatic OpenAPI documentation generation
- Request/response validation
- Asynchronous request handling
- Interactive API testing interface

### **10. Jupyter Notebook Environment**
**Description**: Interactive computing platform for data science and ML development
**Tasks Performed**:
- Exploratory data analysis and visualization
- Iterative model development and testing
- Results documentation and presentation
- Code organization and narrative explanation
- Reproducible research workflow creation

---

## ğŸ“Š **Project Results Summary**

### **Performance Achievements**
- **DistilBERT Model**: 93.51% test accuracy
- **BiLSTM Model**: 91.33% test accuracy
- **Training Environment**: Tesla T4 GPU with 15.8 GB memory
- **Dataset Coverage**: Full 120,000 training samples utilized
- **Training Efficiency**: Both models converged within 5 epochs

### **Technical Contributions**
- Comprehensive comparison of traditional (BiLSTM) vs. modern (DistilBERT) approaches
- Complete end-to-end pipeline from data preprocessing to deployment
- Interactive web interface for real-time text classification
- Detailed performance analysis with statistical significance testing
- Reproducible research with documented methodology and results

---

*This report demonstrates successful implementation of deep learning techniques for natural language processing, achieving state-of-the-art performance on the AG News classification task while providing comprehensive analysis and deployment capabilities.*